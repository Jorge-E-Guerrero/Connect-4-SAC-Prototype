{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from model import SAC\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from pettingzoo.classic import connect_four_v3\n",
    "\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = connect_four_v3.env(render_mode=\"human\")\n",
    "#env = connect_four_v3.env()\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#Numero de iteraciones\n",
    "num_episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de variables\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.0001\n",
    "TAU = 0.0005\n",
    "LR = 3e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = 7\n",
    "\n",
    "# Get the number of state observations\n",
    "observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "#observation_shape = np.array(observation[\"observation\"]).reshape(1,-1)\n",
    "n_observations = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos las 3 instancias\n",
    "\n",
    "sac_agent_0 = SAC(name=\"agent_0\", lr=LR,gamma=GAMMA,tau=TAU,\n",
    "                eps_start=EPS_START,eps_end=EPS_END,eps_dec=EPS_DECAY,\n",
    "                n_observations=n_observations,n_actions=n_actions)\n",
    "\n",
    "sac_agent_1 = SAC(name=\"agent_1\", lr=LR,gamma=GAMMA,tau=TAU,\n",
    "                eps_start=EPS_START,eps_end=EPS_END,eps_dec=EPS_DECAY,\n",
    "                n_observations=n_observations,n_actions=n_actions)\n",
    "\n",
    "sac_agent_a = SAC(name=\"agent_a\", lr=LR,gamma=GAMMA,tau=TAU,\n",
    "                eps_start=EPS_START,eps_end=EPS_END,eps_dec=EPS_DECAY,\n",
    "                n_observations=n_observations,n_actions=n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cargamos los agentes entrenados\n",
    "\n",
    "sac_agent_0.load_model()\n",
    "sac_agent_1.load_model()\n",
    "sac_agent_a.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m             previous_state \u001b[39m=\u001b[39m state\n\u001b[0;32m     70\u001b[0m         env\u001b[39m.\u001b[39mclose()\n\u001b[1;32m---> 71\u001b[0m     sleep(\u001b[39m0.5\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTotal score Competitive SAC Agent: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(total_reward_player_0))\n\u001b[0;32m     77\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTotal score Alone SAC Agent: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(total_reward_player_a))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creamos el primer entorno\n",
    "\n",
    "agent_0_score = []\n",
    "agent_a_score = []\n",
    "\n",
    "\n",
    "\n",
    "total_reward_player_0 = 0\n",
    "total_reward_player_a = 0\n",
    "last_action_player_0 = 0\n",
    "last_action_player_a = 0\n",
    "\n",
    "env.reset()\n",
    "observation, reward, termination, truncation, info = env.last()\n",
    "previous_state = observation\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    #Definicion del primer jugador en cada partida\n",
    "    if(episode % 2 == 1):\n",
    "        for agent in env.agent_iter():\n",
    "            \n",
    "            \n",
    "            \n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "\n",
    "\n",
    "            if termination or truncation:\n",
    "                    # Registramos las recompensas\n",
    "                    total_reward_player_0 = total_reward_player_0 + env.rewards[\"player_0\"]\n",
    "                    total_reward_player_a = total_reward_player_a + env.rewards[\"player_1\"]\n",
    "\n",
    "                    agent_0_score.append(total_reward_player_0)\n",
    "                    agent_a_score.append(total_reward_player_a)\n",
    "\n",
    "                    break\n",
    "            \n",
    "            if(agent == \"player_0\"):\n",
    "                action = sac_agent_0.choose_action(agent,state,env)\n",
    "                env.step(action)\n",
    "            else:\n",
    "                action = sac_agent_a.choose_action(agent,state,env)\n",
    "                env.step(action) \n",
    "            previous_state = state\n",
    "        env.close()\n",
    "    else:\n",
    "        for agent in env.agent_iter():\n",
    "            #print(agent)\n",
    "            state, reward, termination, truncation, info = env.last()\n",
    "\n",
    "\n",
    "            if termination or truncation:\n",
    "                    #print(env.rewards)\n",
    "                    total_reward_player_0 = total_reward_player_0 + env.rewards[\"player_1\"]\n",
    "                    total_reward_player_a = total_reward_player_a + env.rewards[\"player_0\"]\n",
    "\n",
    "                    agent_0_score.append(total_reward_player_0)\n",
    "                    agent_a_score.append(total_reward_player_a)\n",
    "\n",
    "                    break\n",
    "            \n",
    "            if(agent == \"player_1\"):\n",
    "                action = sac_agent_0.choose_action(agent,state,env)\n",
    "                env.step(action)\n",
    "            else:\n",
    "                action = sac_agent_a.choose_action(agent,state,env)\n",
    "                env.step(action) \n",
    "            previous_state = state\n",
    "        env.close()\n",
    "    sleep(0.5)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Total score Competitive SAC Agent: \" + str(total_reward_player_0))\n",
    "print(\"Total score Alone SAC Agent: \" + str(total_reward_player_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Competitive SAC Agent 1 vs. Lone SAC agent\")\n",
    "\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.xlabel(\"Games\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.plot(agent_0_score, label=\"Competitive SAC Agent 1\")\n",
    "plt.plot(agent_a_score, label=\"Lone SAC agent\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
